




















import numpy as np #numerical computation
import pandas as pd #data wrangling
import matplotlib.pyplot as plt #plotting package
#Next line helps with rendering plots
%matplotlib inline
import matplotlib as mpl #add'l plotting functionality
mpl.rcParams['figure.dpi'] = 400 #high res figures
import graphviz #to visualize decision trees





df = pd.read_csv('dataset/default_of_credit_card_clients.csv') #Load the cleaned data
features_response = df.columns.tolist() #Get a list of column names
#Make a list of columns to remove that aren't features or the response variable
items_to_remove = ['ID', 'SEX', 'PAY_2', 'PAY_3',\
'PAY_4', 'PAY_5', 'PAY_6',\
'EDUCATION_CAT', 'graduate school',\
'high school', 'none',\
'others', 'university']
features_response = [item for item in features_response if item not
in items_to_remove]
features_response





from sklearn.model_selection import train_test_split
from sklearn import tree

# Split the data into training and testing sets using the same random seed
X_train, X_test, y_train, y_test = train_test_split(
    df[features_response[:-1]].values,  # Features (all columns except the last one)
    df['default payment next month'].values,  # Target variable
    test_size=0.2, 
    random_state=24,
    stratify=df['default payment next month'].values  # Optional: for stratified sampling
)

print(f"Training set size: {X_train.shape[0]} samples")
print(f"Test set size: {X_test.shape[0]} samples")
print(f"Number of features: {X_train.shape[1]}")
print(f"Training set - Class distribution: {np.bincount(y_train)}")
print(f"Test set - Class distribution: {np.bincount(y_test)}")





# the tree will grow to a depth of at most 2
dt = tree.DecisionTreeClassifier(max_depth=2)
dt.fit(X_train, y_train)





from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(dt, 
          filled=True,
          rounded=True,
          feature_names=features_response[:-1],
          class_names=['Not defaulted', 'Defaulted'],
          fontsize=10,
          max_depth=3)  # Hi·ªÉn th·ªã 3 t·∫ßng ƒë·∫ßu cho d·ªÖ nh√¨n
plt.title('C√¢y quy·∫øt ƒë·ªãnh d·ª± ƒëo√°n v·ª° n·ª£ th·∫ª t√≠n d·ª•ng')
plt.show()








from sklearn.model_selection import GridSearchCV
params = {'max_depth':[1, 2, 4, 6, 8, 10, 12]} #parameters
#Th·ª≠ 7 gi√° tr·ªã ƒë·ªô s√¢u kh√°c nhau t·ª´ c√¢y r·∫•t n√¥ng (depth=1) ƒë·∫øn kh√° s√¢u (depth=12)

dt = tree.DecisionTreeClassifier() #tree modal
cv = GridSearchCV(dt, param_grid=params, scoring='roc_auc',
n_jobs=None, refit=True, cv=4, verbose=1,
error_score=np.nan,
return_train_score=True) # cv is the best model.
#scoring='roc_auc' ‚Üí ∆Øu ti√™n ph√¢n lo·∫°i t·ªët (AUC quan tr·ªçng h∆°n accuracy)

#cv=4 ‚Üí 4-fold cross-validation (chia data th√†nh 4 ph·∫ßn, train 3 test 1)

#refit=True ‚Üí T·ª± ƒë·ªông train l·∫°i model t·ªët nh·∫•t tr√™n to√†n b·ªô data

#verbose=1 ‚Üí Hi·ªÉn th·ªã ti·∫øn tr√¨nh

return_train_score=True ‚Üí Ghi l·∫°i ƒëi·ªÉm train ƒë·ªÉ so s√°nh overfitting
cv.fit(X_train, y_train)


# Sau khi ch·∫°y xong, xem k·∫øt qu·∫£:
print("ƒê·ªô s√¢u t·ªët nh·∫•t:", cv.best_params_)
print("ƒêi·ªÉm AUC t·ªët nh·∫•t:", cv.best_score_)

# Model t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c train t·ª± ƒë·ªông
best_dt = cv.best_estimator_





cv_results_df = pd.DataFrame(cv.cv_results_)

# View the names of the remaining columns in the results DataFrame
cv_results_df.columns

ax = plt.axes()
ax.errorbar(cv_results_df['param_max_depth'],
            cv_results_df['mean_train_score'],
            yerr=cv_results_df['std_train_score']/np.sqrt(4),
            label='Mean $\pm$ 1 SE training scores')
ax.errorbar(cv_results_df['param_max_depth'],
            cv_results_df['mean_test_score'],
            yerr=cv_results_df['std_test_score']/np.sqrt(4),
            label='Mean $\pm$ 1 SE testing scores')
ax.legend()
plt.xlabel('max_depth')
plt.ylabel('ROC AUC')
plt.title('Bi·ªÉu ƒë·ªì ƒë√°nh gi√° hi·ªáu qu·∫£ th·ª±c hi·ªán c√¢y quy·∫øt ƒë·ªãnh v·ªõi c√°c chi·ªÅu s√¢u kh√°c nhau')
plt.show()








from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=10, 
    criterion='gini', 
    max_depth=3,
    min_samples_split=2, 
    min_samples_leaf=1, 
    min_weight_fraction_leaf=0.0,
    max_features='sqrt', 
    max_leaf_nodes=None, 
    min_impurity_decrease=0.0,
    bootstrap=True, 
    oob_score=False, 
    n_jobs=None,
    random_state=4, 
    verbose=0, 
    warm_start=False, 
    class_weight=None
)





# A parameter grid for this exercise in order to search the numbers of
# trees, ranging from 10 to 100 by 10s
rf_params_ex = {'n_estimators': list(range(10, 110, 10))}
cv_rf_ex = GridSearchCV(rf, param_grid=rf_params_ex,
                      scoring='roc_auc', n_jobs=None,
                      refit=True, cv=4, verbose=1,
                      error_score=np.nan,
                      return_train_score=True)

cv_rf_ex.fit(X_train, y_train)


print("S·ªë c√¢y t·ªët nh·∫•t:", cv_rf_ex.best_params_)
print("ƒêi·ªÉm AUC t·ªët nh·∫•t:", cv_rf_ex.best_score_)

# Model t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c train s·∫µn
best_rf = cv_rf_ex.best_estimator_





cv_rf_ex_results_df = pd.DataFrame(cv_rf_ex.cv_results_)
fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(6, 3))
axs[0].plot(cv_rf_ex_results_df['param_n_estimators'],
cv_rf_ex_results_df['mean_fit_time'],
'-o')
axs[0].set_xlabel('Number of trees')
axs[0].set_ylabel('Mean fit time (seconds)')
axs[1].errorbar(cv_rf_ex_results_df['param_n_estimators'],
cv_rf_ex_results_df['mean_test_score'],
yerr=cv_rf_ex_results_df['std_test_score']/np.sqrt(4))
axs[1].set_xlabel('Number of trees')
axs[1].set_ylabel('Mean testing ROC AUC $\pm$ 1 SE ')
plt.tight_layout()
plt.show()





# {'n_estimators': 50}
cv_rf_ex.best_params_
# the feature names and importances
feat_imp_df = pd.DataFrame({
'Importance':cv_rf_ex.best_estimator_.feature_importances_},
index=features_response[:-1])
feat_imp_df.sort_values('Importance', ascending=True).plot.barh()
plt.show()














import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Hi·ªÉn th·ªã ƒë·ªì th·ªã trong notebook
%matplotlib inline
plt.style.use('seaborn-v0_8')





df = pd.read_csv("dataset/train.csv")
df.head()





print(df.info())
print(df.isnull().sum())





# - ƒêi·ªÅn gi√° tr·ªã thi·∫øu cho Age, Embarked
df["Age"].fillna(df["Age"].median(), inplace=True)
df["Embarked"].fillna(df["Embarked"].mode()[0], inplace=True)
df.drop(columns=["Cabin", "Ticket", "Name"], inplace=True)


# - M√£ h√≥a bi·∫øn ph√¢n lo·∫°i
df["Sex"] = df["Sex"].map({"male": 0, "female": 1})
df["Embarked"] = df["Embarked"].map({"S": 0, "C": 1, "Q": 2})





# 5Ô∏è‚É£ Ch·ªçn ƒë·∫∑c tr∆∞ng v√† bi·∫øn m·ª•c ti√™u
X = df.drop(columns=["Survived", "PassengerId"])
y = df["Survived"]





X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Train set: {X_train.shape}, Test set: {X_test.shape}")





dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(X_train, y_train)


y_pred = dt.predict(X_test)
print("üéØ Decision Tree Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))


# Ma tr·∫≠n nh·∫ßm l·∫´n
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Decision Tree")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()


plt.figure(figsize=(18,8))
plot_tree(
    dt, 
    filled=True, 
    feature_names=X.columns, 
    class_names=["Not Survived", "Survived"], 
    fontsize=10
)
plt.title("Decision Tree for Titanic Survival Prediction")
plt.show()





param_grid = {
    "max_depth": [2, 3, 4, 5, 6, 8, 10],
    "criterion": ["gini", "entropy"],
    "min_samples_split": [2, 5, 10]
}
grid_dt = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring="accuracy")
grid_dt.fit(X_train, y_train)

print("‚úÖ Best parameters for Decision Tree:", grid_dt.best_params_)
print("‚úÖ Best score:", grid_dt.best_score_)


from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score
import numpy as np
import matplotlib.pyplot as plt

depths = [2, 3, 4, 6, 8, 10, 12]
train_scores = []
test_scores = []

for d in depths:
    model = DecisionTreeClassifier(max_depth=d, random_state=42)
    # ƒêi·ªÉm train
    model.fit(X_train, y_train)
    train_scores.append(roc_auc_score(y_train, model.predict_proba(X_train)[:,1]))
    # ƒêi·ªÉm test (cross-validation)
    test_scores.append(cross_val_score(model, X_test, y_test, cv=5, scoring='roc_auc').mean())

# V·∫Ω bi·ªÉu ƒë·ªì c√≥ ¬±1 SE
plt.errorbar(depths, train_scores, yerr=np.std(train_scores)/np.sqrt(4),
             label='Mean ¬± 1 SE training scores', marker='o')
plt.errorbar(depths, test_scores, yerr=np.std(test_scores)/np.sqrt(4),
             label='Mean ¬± 1 SE testing scores', marker='o')
plt.xlabel('max_depth')
plt.ylabel('ROC AUC')
plt.legend()
plt.title('K·∫øt qu·∫£ ƒë√°nh gi√° m√¥ h√¨nh v·ªõi c√°c ƒë·ªô s√¢u kh√°c nhau')
plt.show()






rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
rf.fit(X_train, y_train)


# D·ª± ƒëo√°n v√† ƒë√°nh gi√°
y_pred_rf = rf.predict(X_test)
print("üéØ Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt="d", cmap="Greens")
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()


feat_imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values()
plt.figure(figsize=(8,6))
feat_imp.plot(kind="barh")
plt.title("Feature Importance - Random Forest")
plt.xlabel("Importance")
plt.ylabel("Features")
plt.show()



# T·∫°o Random Forest c∆° b·∫£n
rf = RandomForestClassifier(
    criterion='gini', 
    max_depth=5, 
    random_state=42
)

# Thi·∫øt l·∫≠p c√°c gi√° tr·ªã n_estimators ƒë·ªÉ th·ª≠
rf_params = {'n_estimators': list(range(10, 110, 10))}

# D√πng GridSearchCV ƒë·ªÉ ƒë√°nh gi√°
cv_rf = GridSearchCV(
    rf, 
    param_grid=rf_params, 
    scoring='roc_auc', 
    cv=4, 
    return_train_score=True
)
cv_rf.fit(X_train, y_train)

# K·∫øt qu·∫£ ƒë√°nh gi√°
cv_rf_results_df = pd.DataFrame(cv_rf.cv_results_)

# 1Ô∏è‚É£ Bi·ªÉu ƒë·ªì th·ªùi gian hu·∫•n luy·ªán trung b√¨nh
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(cv_rf_results_df['param_n_estimators'], 
            cv_rf_results_df['mean_fit_time'], '-o')
axs[0].set_xlabel('Number of trees')
axs[0].set_ylabel('Mean fit time (seconds)')
axs[0].set_title('Th·ªùi gian hu·∫•n luy·ªán trung b√¨nh')

# 2Ô∏è‚É£ Bi·ªÉu ƒë·ªì ROC AUC trung b√¨nh ¬± 1 SE
axs[1].errorbar(cv_rf_results_df['param_n_estimators'],
                cv_rf_results_df['mean_test_score'],
                yerr=cv_rf_results_df['std_test_score']/np.sqrt(4),
                fmt='-o', capsize=3)
axs[1].set_xlabel('Number of trees')
axs[1].set_ylabel('Mean testing ROC AUC ¬± 1 SE')
axs[1].set_title('Hi·ªáu qu·∫£ m√¥ h√¨nh theo s·ªë l∆∞·ª£ng c√¢y')

plt.tight_layout()
plt.show()















pip install graphviz



# 1. Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import matplotlib as mpl
mpl.rcParams['figure.dpi'] = 400  # tƒÉng ƒë·ªô ph√¢n gi·∫£i ·∫£nh

import graphviz  # d√πng ƒë·ªÉ hi·ªÉn th·ªã c√¢y quy·∫øt ƒë·ªãnh
from sklearn import tree
from sklearn.model_selection import train_test_split





# 2. N·∫°p d·ªØ li·ªáu
df = pd.read_csv("Dataset/diabetes_prediction_dataset.csv")

print("‚úÖ D·ªØ li·ªáu ban ƒë·∫ßu:")
print(df.head())
print("-" * 60+"\n")
print("üìä Th√¥ng tin d·ªØ li·ªáu:")
print(df.info())
print("-" * 60+"\n")
print("üï≥Ô∏è Ki·ªÉm tra gi√° tr·ªã null:")
print(df.isnull().sum())









# 3. M√£ h√≥a c√°c bi·∫øn ph√¢n lo·∫°i
df_encoded = pd.get_dummies(df, columns=['gender', 'smoking_history'], drop_first=False)

print("‚úÖ D·ªØ li·ªáu sau m√£ h√≥a:")
print(df_encoded.head())








# 4. T√°ch ƒë·∫ßu v√†o (X) v√† nh√£n ƒë·∫ßu ra (y)
X = df_encoded.drop("diabetes", axis=1)
y = df_encoded["diabetes"]

# Chia d·ªØ li·ªáu
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"‚úÖ T·∫≠p hu·∫•n luy·ªán: {X_train.shape}")
print(f"‚úÖ T·∫≠p ki·ªÉm tra: {X_test.shape}")









# 5. X√¢y d·ª±ng m√¥ h√¨nh c√¢y quy·∫øt ƒë·ªãnh
dt = tree.DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(X_train, y_train)

print(" M√¥ h√¨nh c√¢y quy·∫øt ƒë·ªãnh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán xong!")






# 6. Xu·∫•t c√¢y quy·∫øt ƒë·ªãnh
dot_data = tree.export_graphviz(
    dt,
    out_file=None,
    filled=True,
    rounded=True,
    feature_names=X.columns,
    class_names=["Kh√¥ng ti·ªÉu ƒë∆∞·ªùng", "Ti·ªÉu ƒë∆∞·ªùng"],
    proportion=True
)

# Hi·ªÉn th·ªã c√¢y
graph = graphviz.Source(dot_data)
graph









from sklearn import tree
from sklearn.model_selection import GridSearchCV
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Khai b√°o t·∫≠p tham s·ªë ƒë·ªÉ d√≤ t√¨m gi√° tr·ªã t·ªëi ∆∞u cho ƒë·ªô s√¢u c·ªßa c√¢y
params = {'max_depth': [1, 2, 4, 6, 8, 10, 12]}

# Kh·ªüi t·∫°o m√¥ h√¨nh Decision Tree
dt = tree.DecisionTreeClassifier(random_state=42)

# C·∫•u h√¨nh GridSearchCV
cv = GridSearchCV(
    dt,
    param_grid=params,
    scoring='roc_auc',      # s·ª≠ d·ª•ng ƒë·ªô ƒëo AUC (ƒë√°nh gi√° kh·∫£ nƒÉng ph√¢n bi·ªát)
    n_jobs=None,
    refit=True,
    cv=4,                   # 4-fold cross validation
    verbose=1,
    error_score=np.nan,
    return_train_score=True # l∆∞u l·∫°i ƒëi·ªÉm hu·∫•n luy·ªán ƒë·ªÉ so s√°nh bias/variance
)

# Hu·∫•n luy·ªán m√¥ h√¨nh
cv.fit(X_train, y_train)

# In ra tham s·ªë t·ªët nh·∫•t v√† ƒëi·ªÉm s·ªë t∆∞∆°ng ·ª©ng
print("Best parameter (max_depth):", cv.best_params_)
print("Best ROC AUC score:", cv.best_score_)









# Chuy·ªÉn k·∫øt qu·∫£ c·ªßa GridSearchCV sang DataFrame
cv_results_df = pd.DataFrame(cv.cv_results_)

# V·∫Ω bi·ªÉu ƒë·ªì so s√°nh hi·ªáu su·∫•t
plt.figure(figsize=(8, 5))
ax = plt.axes()

# ƒê∆∞·ªùng bi·ªÉu di·ªÖn ƒëi·ªÉm train
ax.errorbar(cv_results_df['param_max_depth'],
            cv_results_df['mean_train_score'],
            yerr=cv_results_df['std_train_score'] / np.sqrt(4),
            label='Training (Mean ¬± 1 SE)',
            marker='o')

# ƒê∆∞·ªùng bi·ªÉu di·ªÖn ƒëi·ªÉm test
ax.errorbar(cv_results_df['param_max_depth'],
            cv_results_df['mean_test_score'],
            yerr=cv_results_df['std_test_score'] / np.sqrt(4),
            label='Validation (Mean ¬± 1 SE)',
            marker='s')

# Thi·∫øt l·∫≠p nh√£n
ax.legend()
plt.xlabel('ƒê·ªô s√¢u c·ªßa c√¢y (max_depth)')
plt.ylabel('Gi√° tr·ªã ROC AUC')
plt.title('ƒê√°nh gi√° hi·ªáu qu·∫£ m√¥ h√¨nh Decision Tree v·ªõi c√°c ƒë·ªô s√¢u kh√°c nhau')
plt.grid(True)
plt.show()












from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# T·∫°o m√¥ h√¨nh Random Forest ban ƒë·∫ßu
rf = RandomForestClassifier(
    n_estimators=10,
    criterion='gini',
    max_depth=3,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    random_state=4,
    bootstrap=True
)









# Thi·∫øt l·∫≠p t·∫≠p tham s·ªë c·∫ßn t√¨m
rf_params_ex = {'n_estimators': list(range(10, 110, 10))}

# C·∫•u h√¨nh GridSearchCV
cv_rf_ex = GridSearchCV(
    rf,
    param_grid=rf_params_ex,
    scoring='roc_auc',
    refit=True,
    cv=4,
    verbose=1,
    return_train_score=True
)

# Hu·∫•n luy·ªán m√¥ h√¨nh
cv_rf_ex.fit(X_train, y_train)









# T·∫°o DataFrame ch·ª©a k·∫øt qu·∫£ hu·∫•n luy·ªán
cv_rf_ex_results_df = pd.DataFrame(cv_rf_ex.cv_results_)

# V·∫Ω bi·ªÉu ƒë·ªì
fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))

# Bi·ªÉu ƒë·ªì th·ªùi gian hu·∫•n luy·ªán
axs[0].plot(cv_rf_ex_results_df['param_n_estimators'],
            cv_rf_ex_results_df['mean_fit_time'], '-o')
axs[0].set_xlabel('S·ªë l∆∞·ª£ng c√¢y (n_estimators)')
axs[0].set_ylabel('Th·ªùi gian hu·∫•n luy·ªán trung b√¨nh (gi√¢y)')
axs[0].set_title('Th·ªùi gian hu·∫•n luy·ªán theo s·ªë l∆∞·ª£ng c√¢y')

# Bi·ªÉu ƒë·ªì ƒëi·ªÉm ROC AUC
axs[1].errorbar(cv_rf_ex_results_df['param_n_estimators'],
                cv_rf_ex_results_df['mean_test_score'],
                yerr=cv_rf_ex_results_df['std_test_score']/np.sqrt(4),
                fmt='-o', capsize=3)
axs[1].set_xlabel('S·ªë l∆∞·ª£ng c√¢y (n_estimators)')
axs[1].set_ylabel('ƒêi·ªÉm ROC AUC trung b√¨nh ¬± 1 SE')
axs[1].set_title('Hi·ªáu qu·∫£ m√¥ h√¨nh theo s·ªë l∆∞·ª£ng c√¢y')

plt.tight_layout()
plt.show()









# N·∫øu b·∫°n ƒëang d√πng dataset diabetes_prediction_dataset.csv
features = X_train.columns if isinstance(X_train, pd.DataFrame) else df.columns[:-1]

# L·∫•y ƒë·ªô quan tr·ªçng c·ªßa t·ª´ng feature
feat_imp_df = pd.DataFrame({
    'Importance': cv_rf_ex.best_estimator_.feature_importances_
}, index=features)

# V·∫Ω bi·ªÉu ƒë·ªì m·ª©c ƒë·ªô quan tr·ªçng
feat_imp_df.sort_values('Importance', ascending=True).plot.barh(
    figsize=(8, 6),
    title='M·ª©c ƒë·ªô quan tr·ªçng c·ªßa t·ª´ng ƒë·∫∑c tr∆∞ng'
)
plt.xlabel('ƒê·ªô quan tr·ªçng')
plt.ylabel('ƒê·∫∑c tr∆∞ng')
plt.show()
























import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
# Download&Load d·ªØ li·ªáu iris t·ª´ datasets c·ªßa scikit-learn
iris = datasets.load_iris()
# Hi·ªÉn th·ªã m√¥ ta d·ªØ li·ªáu, ch·ªâ c√≥ trong c√°c b·ªô d·ªØ li·ªáu chu·∫©n v√† m·ªü ƒë·ªÉ h·ªçc t·∫≠p v√† nghi√™n c·ª©u
print(iris.DESCR)
# T·ª´ t·∫≠p d·ªØ li·ªáu ban ƒë·∫ßu, t√°ch l·∫•y ma tr·∫≠n bi·ªÉu di·ªÖn c√°c ƒë·∫∑c tr∆∞ng v√†nh√£n.
data = iris.data
target = iris.target
# TODO: Chia d·ªØ li·ªáu v√† nh√£n th√†nh 2 t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán v√† d·ªØ li·ªáuki·ªÉm tra theo t·ªâ l·ªá 80:20
X_train, X_test, y_train, y_test = train_test_split(data, target,
test_size
= 0.2, random_state=101)








from sklearn import svm
# kh·ªüi t·∫°o m√¥ h√¨nh ph√¢n l·ªõp
clf = svm.SVC()
# S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c 'fit' ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi d·ªØ li·ªáu hu·∫•n luy·ªán v√† nh√£n hu·∫•n luy·ªán
# fit (X,Y) v·ªõi X l√† t·∫≠p c√°c ƒë·ªëi t∆∞·ª£ng, Y l√† t·∫≠p nh√£n t∆∞∆°ng ·ª©ng c·ªßa ƒë·ªëi t∆∞·ª£ng.
clf.fit(X_train, y_train)








# T√≠nh ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra
train_acc = clf.score(X_train,y_train)
val_acc = clf.score(X_test,y_test)
print('Training accuracy: {}'.format(train_acc))
print('Validation accuracy: {}'.format(val_acc))








# best_svm, best_val_acc v√† best_kernel l·∫ßn l∆∞·ª£t l√† c√°c bi·∫øn l∆∞u m√¥ h√¨nh t·ªët nh·∫•t,
# ƒë·ªô ch√≠nh x√°c cao nh·∫•t tr√™n t·∫≠p ki·ªÉm tra v√† kernel t·ªët nh·∫•t
kernels = ['linear', 'poly', 'rbf', 'sigmoid']
best_svm = None
best_val_acc = -1
best_kernel = None
# Hu·∫•n luy·ªán c√°c m√¥ h√¨nh d·ª±a tr√™n d·ªØ li·ªáu hu·∫•n luy·ªán v√† tham s·ªë kernel
# T√≠nh to√°n ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra ƒë·ªÉ t√¨m ƒë∆∞·ª£c m√¥ h√¨nh t·ªët nh·∫•t
for i in range(4):
    clf = svm.SVC(kernel=kernels[i], probability=True)
    clf.fit(X_train, y_train)
    tmp_val_acc = clf.score(X_test, y_test)
    if (tmp_val_acc > best_val_acc):
        best_val_acc = tmp_val_acc
        best_svm = clf
        best_kernel = kernels[i]
# Hi·ªÉn th·ªã m√¥ h√¨nh t·ªët nh·∫•t c√πng v·ªõi ƒë·ªô ch√≠nh x√°c
print("Best validation accuracy : {} with kernel: {}".format(best_val_acc,
best_kernel))
# M√¥ h√¨nh t·ªët nh·∫•t c·ªßa b·∫°n n√™n c√≥ ƒë·ªô ch√≠nh x√°c x·∫•p x·ªâ 86,67%


















import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

from sklearn.datasets import load_digits
digits = load_digits(n_class=10)








#th√¥ng tin to√†n b·ªô d·ªØ li·ªáu ƒë√£ t·∫£i v·ªÅ digits
#xem th√¥ng tin c·ªßa m·ªôt h√¨nh d∆∞·ªõi d·∫°ng ma tr·∫≠n 8 x 8
digits['data'][0].reshape(8,8)
#xem th√¥ng tin c·ªßa m·ªôt h√¨nh d∆∞·ªõi d·∫°ng m·∫£ng
digits['data'][0]
#xem th√¥ng tin 9 nh√£n ƒë·∫ßu ti√™n
digits['target'][0:9]








import matplotlib.pyplot as plt
from sklearn.datasets import load_digits

# N·∫°p d·ªØ li·ªáu digits
digits = load_digits()

# M·ªói ·∫£nh l√† ma tr·∫≠n 8x8
fig, ax = plt.subplots(8, 8, figsize=(8, 8))

for i, axi in enumerate(ax.flat):
    axi.imshow(digits.images[i], cmap='binary')  # Hi·ªÉn th·ªã ·∫£nh m·ª©c x√°m
    axi.set(xticks=[], yticks=[])                # ·∫®n tr·ª•c

plt.tight_layout()
plt.show()









# H√†m v·∫Ω 1 ·∫£nh c√≥ k√≠ch th∆∞·ªõc 8 x 8 (·∫£nh l·∫•y t·ª´ ma images)
def view_digit(index):
    plt.imshow(digits.images[index] , cmap = plt.cm.gray_r)
    plt.title('Orignal it is: '+ str(digits.target[index]))
    plt.show()
# v·∫Ω ·∫£nh ·ªü v·ªã tr√≠ th·ª© 4
view_digit(4)











# Th·ª±c hi·ªán import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh SVM
# Th·ª±c hi·ªán b∆∞·ªõc 1 c·ªßa nhi·ªám v·ª• 1
from sklearn import svm
main_data = digits['data']
targets = digits['target']
svc = svm.SVC(gamma=0.001 , C = 100)
# GAMMA is a parameter for non linear hyperplanes.
# The higher the gamma value it tries to exactly fit the training data set
# C is the penalty parameter of the error term.
# It controls the trade off between smooth decision boundary and classifying the training points correctly.
svc.fit(main_data[:1500] , targets[:1500])
predictions = svc.predict(main_data[1501:])
# list(zip(predictions , targets[1501:]))








from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# T·∫°o confusion matrix
cm = confusion_matrix(predictions, targets[1501:])

# ƒê∆∞a v·ªÅ DataFrame
conf_matrix = pd.DataFrame(data=cm)

# Thi·∫øt l·∫≠p giao di·ªán v√† ki·ªÉu hi·ªÉn th·ªã
sns.set(font_scale=1.0)
sns.set_style("dark")

# V·∫Ω heatmap
plt.figure(figsize=(8, 5))
ax = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="YlGnBu", cbar=True)

# Th√™m ti√™u ƒë·ªÅ v√† ƒë·ªãnh d·∫°ng
plt.title("Confusion Matrix tr√¨nh b√†y d∆∞·ªõi d·∫°ng heatmap", fontsize=12, pad=10)
plt.xlabel("")
plt.ylabel("")

plt.tight_layout()

# Ch·ªâ g·ªçi plt.show() M·ªòT L·∫¶N, sau c√πng
plt.show()

# Kh√¥ng ƒë·ªÉ cell k·∫øt th√∫c b·∫±ng m·ªôt ƒë·ªëi t∆∞·ª£ng (vd. 'conf_matrix' ho·∫∑c 'ax')
# => tr√°nh Jupyter t·ª± hi·ªÉn th·ªã l·∫°i ma tr·∫≠n th·ª© 2









from sklearn.metrics import classification_report
print(classification_report(predictions, targets[1501:]))



