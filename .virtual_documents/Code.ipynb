





pip install graphviz



# 1. Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import matplotlib as mpl
mpl.rcParams['figure.dpi'] = 400  # tƒÉng ƒë·ªô ph√¢n gi·∫£i ·∫£nh

import graphviz  # d√πng ƒë·ªÉ hi·ªÉn th·ªã c√¢y quy·∫øt ƒë·ªãnh
from sklearn import tree
from sklearn.model_selection import train_test_split








# 2. N·∫°p d·ªØ li·ªáu
df = pd.read_csv("Dataset/diabetes_prediction_dataset.csv")

print("‚úÖ D·ªØ li·ªáu ban ƒë·∫ßu:")
print(df.head())
print("-" * 60+"\n")
print("üìä Th√¥ng tin d·ªØ li·ªáu:")
print(df.info())
print("-" * 60+"\n")
print("üï≥Ô∏è Ki·ªÉm tra gi√° tr·ªã null:")
print(df.isnull().sum())









# 3. M√£ h√≥a c√°c bi·∫øn ph√¢n lo·∫°i
df_encoded = pd.get_dummies(df, columns=['gender', 'smoking_history'], drop_first=False)

print("‚úÖ D·ªØ li·ªáu sau m√£ h√≥a:")
print(df_encoded.head())








# 4. T√°ch ƒë·∫ßu v√†o (X) v√† nh√£n ƒë·∫ßu ra (y)
X = df_encoded.drop("diabetes", axis=1)
y = df_encoded["diabetes"]

# Chia d·ªØ li·ªáu
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"‚úÖ T·∫≠p hu·∫•n luy·ªán: {X_train.shape}")
print(f"‚úÖ T·∫≠p ki·ªÉm tra: {X_test.shape}")









# 5. X√¢y d·ª±ng m√¥ h√¨nh c√¢y quy·∫øt ƒë·ªãnh
dt = tree.DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(X_train, y_train)

print("‚úÖ M√¥ h√¨nh c√¢y quy·∫øt ƒë·ªãnh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán xong!")






# 6. Xu·∫•t c√¢y quy·∫øt ƒë·ªãnh
dot_data = tree.export_graphviz(
    dt,
    out_file=None,
    filled=True,
    rounded=True,
    feature_names=X.columns,
    class_names=["Kh√¥ng ti·ªÉu ƒë∆∞·ªùng", "Ti·ªÉu ƒë∆∞·ªùng"],
    proportion=True
)

# Hi·ªÉn th·ªã c√¢y
graph = graphviz.Source(dot_data)
graph






























import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
# Download&Load d·ªØ li·ªáu iris t·ª´ datasets c·ªßa scikit-learn
iris = datasets.load_iris()
# Hi·ªÉn th·ªã m√¥ ta d·ªØ li·ªáu, ch·ªâ c√≥ trong c√°c b·ªô d·ªØ li·ªáu chu·∫©n v√† m·ªü ƒë·ªÉ h·ªçc t·∫≠p v√† nghi√™n c·ª©u
print(iris.DESCR)
# T·ª´ t·∫≠p d·ªØ li·ªáu ban ƒë·∫ßu, t√°ch l·∫•y ma tr·∫≠n bi·ªÉu di·ªÖn c√°c ƒë·∫∑c tr∆∞ng v√†nh√£n.
data = iris.data
target = iris.target
# TODO: Chia d·ªØ li·ªáu v√† nh√£n th√†nh 2 t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán v√† d·ªØ li·ªáuki·ªÉm tra theo t·ªâ l·ªá 80:20
X_train, X_test, y_train, y_test = train_test_split(data, target,
test_size
= 0.2, random_state=101)








from sklearn import svm
# kh·ªüi t·∫°o m√¥ h√¨nh ph√¢n l·ªõp
clf = svm.SVC()
# S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c 'fit' ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi d·ªØ li·ªáu hu·∫•n luy·ªán v√† nh√£n hu·∫•n luy·ªán
# fit (X,Y) v·ªõi X l√† t·∫≠p c√°c ƒë·ªëi t∆∞·ª£ng, Y l√† t·∫≠p nh√£n t∆∞∆°ng ·ª©ng c·ªßa ƒë·ªëi t∆∞·ª£ng.
clf.fit(X_train, y_train)








# T√≠nh ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra
train_acc = clf.score(X_train,y_train)
val_acc = clf.score(X_test,y_test)
print('Training accuracy: {}'.format(train_acc))
print('Validation accuracy: {}'.format(val_acc))








# best_svm, best_val_acc v√† best_kernel l·∫ßn l∆∞·ª£t l√† c√°c bi·∫øn l∆∞u m√¥ h√¨nh t·ªët nh·∫•t,
# ƒë·ªô ch√≠nh x√°c cao nh·∫•t tr√™n t·∫≠p ki·ªÉm tra v√† kernel t·ªët nh·∫•t
kernels = ['linear', 'poly', 'rbf', 'sigmoid']
best_svm = None
best_val_acc = -1
best_kernel = None
# Hu·∫•n luy·ªán c√°c m√¥ h√¨nh d·ª±a tr√™n d·ªØ li·ªáu hu·∫•n luy·ªán v√† tham s·ªë kernel
# T√≠nh to√°n ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra ƒë·ªÉ t√¨m ƒë∆∞·ª£c m√¥ h√¨nh t·ªët nh·∫•t
for i in range(4):
    clf = svm.SVC(kernel=kernels[i], probability=True)
    clf.fit(X_train, y_train)
    tmp_val_acc = clf.score(X_test, y_test)
    if (tmp_val_acc > best_val_acc):
        best_val_acc = tmp_val_acc
        best_svm = clf
        best_kernel = kernels[i]
# Hi·ªÉn th·ªã m√¥ h√¨nh t·ªët nh·∫•t c√πng v·ªõi ƒë·ªô ch√≠nh x√°c
print("Best validation accuracy : {} with kernel: {}".format(best_val_acc,
best_kernel))
# M√¥ h√¨nh t·ªët nh·∫•t c·ªßa b·∫°n n√™n c√≥ ƒë·ªô ch√≠nh x√°c x·∫•p x·ªâ 86,67%


















import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

from sklearn.datasets import load_digits
digits = load_digits(n_class=10)








#th√¥ng tin to√†n b·ªô d·ªØ li·ªáu ƒë√£ t·∫£i v·ªÅ digits
#xem th√¥ng tin c·ªßa m·ªôt h√¨nh d∆∞·ªõi d·∫°ng ma tr·∫≠n 8 x 8
digits['data'][0].reshape(8,8)
#xem th√¥ng tin c·ªßa m·ªôt h√¨nh d∆∞·ªõi d·∫°ng m·∫£ng
digits['data'][0]
#xem th√¥ng tin 9 nh√£n ƒë·∫ßu ti√™n
digits['target'][0:9]








import matplotlib.pyplot as plt
from sklearn.datasets import load_digits

# N·∫°p d·ªØ li·ªáu digits
digits = load_digits()

# M·ªói ·∫£nh l√† ma tr·∫≠n 8x8
fig, ax = plt.subplots(8, 8, figsize=(8, 8))

for i, axi in enumerate(ax.flat):
    axi.imshow(digits.images[i], cmap='binary')  # Hi·ªÉn th·ªã ·∫£nh m·ª©c x√°m
    axi.set(xticks=[], yticks=[])                # ·∫®n tr·ª•c

plt.tight_layout()
plt.show()









# H√†m v·∫Ω 1 ·∫£nh c√≥ k√≠ch th∆∞·ªõc 8 x 8 (·∫£nh l·∫•y t·ª´ ma images)
def view_digit(index):
    plt.imshow(digits.images[index] , cmap = plt.cm.gray_r)
    plt.title('Orignal it is: '+ str(digits.target[index]))
    plt.show()
# v·∫Ω ·∫£nh ·ªü v·ªã tr√≠ th·ª© 4
view_digit(4)











# Th·ª±c hi·ªán import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh SVM
# Th·ª±c hi·ªán b∆∞·ªõc 1 c·ªßa nhi·ªám v·ª• 1
from sklearn import svm
main_data = digits['data']
targets = digits['target']
svc = svm.SVC(gamma=0.001 , C = 100)
# GAMMA is a parameter for non linear hyperplanes.
# The higher the gamma value it tries to exactly fit the training data set
# C is the penalty parameter of the error term.
# It controls the trade off between smooth decision boundary and classifying the training points correctly.
svc.fit(main_data[:1500] , targets[:1500])
predictions = svc.predict(main_data[1501:])
# list(zip(predictions , targets[1501:]))








from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# T·∫°o confusion matrix
cm = confusion_matrix(predictions, targets[1501:])

# ƒê∆∞a v·ªÅ DataFrame
conf_matrix = pd.DataFrame(data=cm)

# Thi·∫øt l·∫≠p giao di·ªán v√† ki·ªÉu hi·ªÉn th·ªã
sns.set(font_scale=1.0)
sns.set_style("dark")

# V·∫Ω heatmap
plt.figure(figsize=(8, 5))
ax = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="YlGnBu", cbar=True)

# Th√™m ti√™u ƒë·ªÅ v√† ƒë·ªãnh d·∫°ng
plt.title("Confusion Matrix tr√¨nh b√†y d∆∞·ªõi d·∫°ng heatmap", fontsize=12, pad=10)
plt.xlabel("")
plt.ylabel("")

plt.tight_layout()

# Ch·ªâ g·ªçi plt.show() M·ªòT L·∫¶N, sau c√πng
plt.show()

# Kh√¥ng ƒë·ªÉ cell k·∫øt th√∫c b·∫±ng m·ªôt ƒë·ªëi t∆∞·ª£ng (vd. 'conf_matrix' ho·∫∑c 'ax')
# => tr√°nh Jupyter t·ª± hi·ªÉn th·ªã l·∫°i ma tr·∫≠n th·ª© 2









from sklearn.metrics import classification_report
print(classification_report(predictions, targets[1501:]))












