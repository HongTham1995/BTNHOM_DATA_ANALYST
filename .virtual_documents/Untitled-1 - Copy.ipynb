



































import numpy as np #numerical computation
import pandas as pd #data wrangling
import matplotlib.pyplot as plt #plotting package
#Next line helps with rendering plots
%matplotlib inline
import matplotlib as mpl #add'l plotting functionality
mpl.rcParams['figure.dpi'] = 400 #high res figures
import graphviz #to visualize decision trees





df = pd.read_csv('dataset/default_of_credit_card_clients.csv') #Load the cleaned data
features_response = df.columns.tolist() #Get a list of column names
#Make a list of columns to remove that aren't features or the response variable
items_to_remove = ['ID', 'SEX', 'PAY_2', 'PAY_3',\
'PAY_4', 'PAY_5', 'PAY_6',\
'EDUCATION_CAT', 'graduate school',\
'high school', 'none',\
'others', 'university']
features_response = [item for item in features_response if item not
in items_to_remove]
features_response





from sklearn.model_selection import train_test_split
from sklearn import tree

# Split the data into training and testing sets using the same random seed
X_train, X_test, y_train, y_test = train_test_split(
    df[features_response[:-1]].values,  # Features (all columns except the last one)
    df['default payment next month'].values,  # Target variable
    test_size=0.2, 
    random_state=24,
    stratify=df['default payment next month'].values  # Optional: for stratified sampling
)

print(f"Training set size: {X_train.shape[0]} samples")
print(f"Test set size: {X_test.shape[0]} samples")
print(f"Number of features: {X_train.shape[1]}")
print(f"Training set - Class distribution: {np.bincount(y_train)}")
print(f"Test set - Class distribution: {np.bincount(y_test)}")





# the tree will grow to a depth of at most 2
dt = tree.DecisionTreeClassifier(max_depth=2)
dt.fit(X_train, y_train)





from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(dt, 
          filled=True,
          rounded=True,
          feature_names=features_response[:-1],
          class_names=['Not defaulted', 'Defaulted'],
          fontsize=10,
          max_depth=3)  # Hi·ªÉn th·ªã 3 t·∫ßng ƒë·∫ßu cho d·ªÖ nh√¨n
plt.title('C√¢y quy·∫øt ƒë·ªãnh d·ª± ƒëo√°n v·ª° n·ª£ th·∫ª t√≠n d·ª•ng')
plt.show()








from sklearn.model_selection import GridSearchCV
params = {'max_depth':[1, 2, 4, 6, 8, 10, 12]} #parameters
#Th·ª≠ 7 gi√° tr·ªã ƒë·ªô s√¢u kh√°c nhau t·ª´ c√¢y r·∫•t n√¥ng (depth=1) ƒë·∫øn kh√° s√¢u (depth=12)

dt = tree.DecisionTreeClassifier() #tree modal
cv = GridSearchCV(dt, param_grid=params, scoring='roc_auc',
n_jobs=None, refit=True, cv=4, verbose=1,
error_score=np.nan,
return_train_score=True) # cv is the best model.
#scoring='roc_auc' ‚Üí ∆Øu ti√™n ph√¢n lo·∫°i t·ªët (AUC quan tr·ªçng h∆°n accuracy)

#cv=4 ‚Üí 4-fold cross-validation (chia data th√†nh 4 ph·∫ßn, train 3 test 1)

#refit=True ‚Üí T·ª± ƒë·ªông train l·∫°i model t·ªët nh·∫•t tr√™n to√†n b·ªô data

#verbose=1 ‚Üí Hi·ªÉn th·ªã ti·∫øn tr√¨nh

return_train_score=True ‚Üí Ghi l·∫°i ƒëi·ªÉm train ƒë·ªÉ so s√°nh overfitting
cv.fit(X_train, y_train)


# Sau khi ch·∫°y xong, xem k·∫øt qu·∫£:
print("ƒê·ªô s√¢u t·ªët nh·∫•t:", cv.best_params_)
print("ƒêi·ªÉm AUC t·ªët nh·∫•t:", cv.best_score_)

# Model t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c train t·ª± ƒë·ªông
best_dt = cv.best_estimator_





cv_results_df = pd.DataFrame(cv.cv_results_)

# View the names of the remaining columns in the results DataFrame
cv_results_df.columns

ax = plt.axes()
ax.errorbar(cv_results_df['param_max_depth'],
            cv_results_df['mean_train_score'],
            yerr=cv_results_df['std_train_score']/np.sqrt(4),
            label='Mean $\pm$ 1 SE training scores')
ax.errorbar(cv_results_df['param_max_depth'],
            cv_results_df['mean_test_score'],
            yerr=cv_results_df['std_test_score']/np.sqrt(4),
            label='Mean $\pm$ 1 SE testing scores')
ax.legend()
plt.xlabel('max_depth')
plt.ylabel('ROC AUC')
plt.title('Bi·ªÉu ƒë·ªì ƒë√°nh gi√° hi·ªáu qu·∫£ th·ª±c hi·ªán c√¢y quy·∫øt ƒë·ªãnh v·ªõi c√°c chi·ªÅu s√¢u kh√°c nhau')
plt.show()








from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=10, 
    criterion='gini', 
    max_depth=3,
    min_samples_split=2, 
    min_samples_leaf=1, 
    min_weight_fraction_leaf=0.0,
    max_features='sqrt', 
    max_leaf_nodes=None, 
    min_impurity_decrease=0.0,
    bootstrap=True, 
    oob_score=False, 
    n_jobs=None,
    random_state=4, 
    verbose=0, 
    warm_start=False, 
    class_weight=None
)





# A parameter grid for this exercise in order to search the numbers of
# trees, ranging from 10 to 100 by 10s
rf_params_ex = {'n_estimators': list(range(10, 110, 10))}
cv_rf_ex = GridSearchCV(rf, param_grid=rf_params_ex,
                      scoring='roc_auc', n_jobs=None,
                      refit=True, cv=4, verbose=1,
                      error_score=np.nan,
                      return_train_score=True)

cv_rf_ex.fit(X_train, y_train)


print("S·ªë c√¢y t·ªët nh·∫•t:", cv_rf_ex.best_params_)
print("ƒêi·ªÉm AUC t·ªët nh·∫•t:", cv_rf_ex.best_score_)

# Model t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c train s·∫µn
best_rf = cv_rf_ex.best_estimator_





cv_rf_ex_results_df = pd.DataFrame(cv_rf_ex.cv_results_)
fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(6, 3))
axs[0].plot(cv_rf_ex_results_df['param_n_estimators'],
cv_rf_ex_results_df['mean_fit_time'],
'-o')
axs[0].set_xlabel('Number of trees')
axs[0].set_ylabel('Mean fit time (seconds)')
axs[1].errorbar(cv_rf_ex_results_df['param_n_estimators'],
cv_rf_ex_results_df['mean_test_score'],
yerr=cv_rf_ex_results_df['std_test_score']/np.sqrt(4))
axs[1].set_xlabel('Number of trees')
axs[1].set_ylabel('Mean testing ROC AUC $\pm$ 1 SE ')
plt.tight_layout()
plt.show()





# {'n_estimators': 50}
cv_rf_ex.best_params_
# the feature names and importances
feat_imp_df = pd.DataFrame({
'Importance':cv_rf_ex.best_estimator_.feature_importances_},
index=features_response[:-1])
feat_imp_df.sort_values('Importance', ascending=True).plot.barh()
plt.show()














import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Hi·ªÉn th·ªã ƒë·ªì th·ªã trong notebook
%matplotlib inline
plt.style.use('seaborn-v0_8')





df = pd.read_csv("dataset/train.csv")
df.head()





print(df.info())
print(df.isnull().sum())





# - ƒêi·ªÅn gi√° tr·ªã thi·∫øu cho Age, Embarked
df["Age"].fillna(df["Age"].median(), inplace=True)
df["Embarked"].fillna(df["Embarked"].mode()[0], inplace=True)
df.drop(columns=["Cabin", "Ticket", "Name"], inplace=True)


# - M√£ h√≥a bi·∫øn ph√¢n lo·∫°i
df["Sex"] = df["Sex"].map({"male": 0, "female": 1})
df["Embarked"] = df["Embarked"].map({"S": 0, "C": 1, "Q": 2})





# 5Ô∏è‚É£ Ch·ªçn ƒë·∫∑c tr∆∞ng v√† bi·∫øn m·ª•c ti√™u
X = df.drop(columns=["Survived", "PassengerId"])
y = df["Survived"]





X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Train set: {X_train.shape}, Test set: {X_test.shape}")





dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(X_train, y_train)


y_pred = dt.predict(X_test)
print("üéØ Decision Tree Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))


# Ma tr·∫≠n nh·∫ßm l·∫´n
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Decision Tree")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()


plt.figure(figsize=(18,8))
plot_tree(
    dt, 
    filled=True, 
    feature_names=X.columns, 
    class_names=["Not Survived", "Survived"], 
    fontsize=10
)
plt.title("Decision Tree for Titanic Survival Prediction")
plt.show()





param_grid = {
    "max_depth": [2, 3, 4, 5, 6, 8, 10],
    "criterion": ["gini", "entropy"],
    "min_samples_split": [2, 5, 10]
}
grid_dt = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring="accuracy")
grid_dt.fit(X_train, y_train)

print("‚úÖ Best parameters for Decision Tree:", grid_dt.best_params_)
print("‚úÖ Best score:", grid_dt.best_score_)


from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score
import numpy as np
import matplotlib.pyplot as plt

depths = [2, 3, 4, 6, 8, 10, 12]
train_scores = []
test_scores = []

for d in depths:
    model = DecisionTreeClassifier(max_depth=d, random_state=42)
    # ƒêi·ªÉm train
    model.fit(X_train, y_train)
    train_scores.append(roc_auc_score(y_train, model.predict_proba(X_train)[:,1]))
    # ƒêi·ªÉm test (cross-validation)
    test_scores.append(cross_val_score(model, X_test, y_test, cv=5, scoring='roc_auc').mean())

# V·∫Ω bi·ªÉu ƒë·ªì c√≥ ¬±1 SE
plt.errorbar(depths, train_scores, yerr=np.std(train_scores)/np.sqrt(4),
             label='Mean ¬± 1 SE training scores', marker='o')
plt.errorbar(depths, test_scores, yerr=np.std(test_scores)/np.sqrt(4),
             label='Mean ¬± 1 SE testing scores', marker='o')
plt.xlabel('max_depth')
plt.ylabel('ROC AUC')
plt.legend()
plt.title('K·∫øt qu·∫£ ƒë√°nh gi√° m√¥ h√¨nh v·ªõi c√°c ƒë·ªô s√¢u kh√°c nhau')
plt.show()






rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
rf.fit(X_train, y_train)


# D·ª± ƒëo√°n v√† ƒë√°nh gi√°
y_pred_rf = rf.predict(X_test)
print("üéØ Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt="d", cmap="Greens")
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()


feat_imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values()
plt.figure(figsize=(8,6))
feat_imp.plot(kind="barh")
plt.title("Feature Importance - Random Forest")
plt.xlabel("Importance")
plt.ylabel("Features")
plt.show()



# T·∫°o Random Forest c∆° b·∫£n
rf = RandomForestClassifier(
    criterion='gini', 
    max_depth=5, 
    random_state=42
)

# Thi·∫øt l·∫≠p c√°c gi√° tr·ªã n_estimators ƒë·ªÉ th·ª≠
rf_params = {'n_estimators': list(range(10, 110, 10))}

# D√πng GridSearchCV ƒë·ªÉ ƒë√°nh gi√°
cv_rf = GridSearchCV(
    rf, 
    param_grid=rf_params, 
    scoring='roc_auc', 
    cv=4, 
    return_train_score=True
)
cv_rf.fit(X_train, y_train)

# K·∫øt qu·∫£ ƒë√°nh gi√°
cv_rf_results_df = pd.DataFrame(cv_rf.cv_results_)

# 1Ô∏è‚É£ Bi·ªÉu ƒë·ªì th·ªùi gian hu·∫•n luy·ªán trung b√¨nh
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(cv_rf_results_df['param_n_estimators'], 
            cv_rf_results_df['mean_fit_time'], '-o')
axs[0].set_xlabel('Number of trees')
axs[0].set_ylabel('Mean fit time (seconds)')
axs[0].set_title('Th·ªùi gian hu·∫•n luy·ªán trung b√¨nh')

# 2Ô∏è‚É£ Bi·ªÉu ƒë·ªì ROC AUC trung b√¨nh ¬± 1 SE
axs[1].errorbar(cv_rf_results_df['param_n_estimators'],
                cv_rf_results_df['mean_test_score'],
                yerr=cv_rf_results_df['std_test_score']/np.sqrt(4),
                fmt='-o', capsize=3)
axs[1].set_xlabel('Number of trees')
axs[1].set_ylabel('Mean testing ROC AUC ¬± 1 SE')
axs[1].set_title('Hi·ªáu qu·∫£ m√¥ h√¨nh theo s·ªë l∆∞·ª£ng c√¢y')

plt.tight_layout()
plt.show()




